# 主成分分析（Principal Components Analysis，PCA）
> 设在n维空间有m个样本点$X=\lbrace{x_1,x_2,...,a_m}\rbrace$，对这些数据进行压缩，让其投影到k维空间，使得丢失信息最少

求投影的新坐标系$W=\lbrace{w_1,w_2,...,w_k}\rbrace$，$w_i$是n维向量，且是标准正交基向量，满足：
* $\Vert{w_i}\Vert_2=1$
* $w_iw_j=0$

矩阵$W$是一个$n * k$维的正交矩阵，也叫做投影矩阵，数据投影到k维空间的结果$Z=W^TX$，$Z$是$m * k$的矩阵，数据被压缩到k维空间。
> 恢复$Z$到$X$时仅需要$X^*=WZ=WW^TX=IX$即可

$X^*$与$X$的距离最小时，W即为求解目标，所以只需求解：

* $$\min_{W}{\Vert{X-X^*}\Vert_F^2} = \min_{W}{\Vert{ X-WW^TX}\Vert_F^2}$$
* $$s.t.WW^T=I$$

根据上节介绍，A的迹与A的F范数有关：$\Vert{A}\Vert_F=\sqrt{tr(AA^T)}$

$$\min_{W}{\Vert{X-WW^TX}\Vert_F^2} = \min_{W}(tr((X-WW^TX)^T(X-WW^TX)))$$

> 展开后，由于$WW^T=I$，且与$W$无关的项不会影响优化结果，可得
> $$\min_{W}\Vert{X-WW^TX}\Vert_F^2 = \max_{W}tr(X^TWW^TX)$$

利用拉格朗日乘子法求解后，解得：

$$XX^TW = λW$$

> * $W$是由$XX^T$的特征向量构成的特征矩阵；
> * 该式为矩阵的SVD分解式，W={w1，w2，...，wk}，wi是XX'的第i大特征值对应的特征向量；

## 原理
![](./img/pca_1.png)  

PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。 

还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。

$$A_{m * n}P_{n * n}=\tilde{A}_{m * n}$$

而将一个m * n的矩阵A变换成一个m * r的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了$(r < n)$，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：

$$A_{m * n}P_{n * r}=\tilde{A}_{m * r}$$

但是这个怎么和SVD扯上关系呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：

$${{A} _{m * n}}\approx{U _{m * r}\Sigma _{r * r} V _{r * n}^T}$$

在矩阵的两边同时乘上一个矩阵V，由于V是一个正交的矩阵，所以V转置乘以V得到单位阵I，所以可以化成后面的式子

$${{A} _{m * n}V _{r * n}}\approx{U _{m * r}\Sigma _{r * r} V _{r * n}^TV _{r * n}}$$

$${{A} _{m * n}V _{r * n}}\approx{U _{m * r}\Sigma _{r * r}}$$

将后面的式子与A * P那个m * n的矩阵变换为m * r的矩阵的式子对照看看，在这里，其实V就是P，也就是一个变化的向量。这里是将一个m * n 的矩阵压缩到一个m * r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子：

$$P_{r * m}A_{m * n}=\tilde{A}_{r * n}$$

这样就从一个m行的矩阵压缩到一个r行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以U的转置U'

$${{U} _{m * r}^TA _{m * n}}\approx{\Sigma _{r * r}V _{r * n}^T}$$

这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。

[返回目录](../CONTENTS.md)